{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43932137",
   "metadata": {},
   "source": [
    "# Part Two\n",
    "## 1.Loading NSL_KDD DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "051db5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
      "0         0           tcp  ftp_data   SF        491          0     0   \n",
      "1         0           udp     other   SF        146          0     0   \n",
      "2         0           tcp   private   S0          0          0     0   \n",
      "3         0           tcp      http   SF        232       8153     0   \n",
      "4         0           tcp      http   SF        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
      "0               0       0    0  ...                    0.17   \n",
      "1               0       0    0  ...                    0.00   \n",
      "2               0       0    0  ...                    0.10   \n",
      "3               0       0    0  ...                    1.00   \n",
      "4               0       0    0  ...                    1.00   \n",
      "\n",
      "   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
      "0                    0.03                         0.17   \n",
      "1                    0.60                         0.88   \n",
      "2                    0.05                         0.00   \n",
      "3                    0.00                         0.03   \n",
      "4                    0.00                         0.00   \n",
      "\n",
      "   dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
      "0                         0.00                  0.00   \n",
      "1                         0.00                  0.00   \n",
      "2                         0.00                  1.00   \n",
      "3                         0.04                  0.03   \n",
      "4                         0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
      "0                      0.00                  0.05                      0.00   \n",
      "1                      0.00                  0.00                      0.00   \n",
      "2                      1.00                  0.00                      0.00   \n",
      "3                      0.01                  0.00                      0.01   \n",
      "4                      0.00                  0.00                      0.00   \n",
      "\n",
      "     label  difficulty  \n",
      "0   normal          20  \n",
      "1   normal          15  \n",
      "2  neptune          19  \n",
      "3   normal          21  \n",
      "4   normal          21  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # importig pandas to laod the file into data frame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "columns = [ \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\n",
    "\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\n",
    "\"num_root\",\"num_file_creations\",\"num_shells\",\"num_access_files\",\n",
    "\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\n",
    "\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\n",
    "\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\n",
    "\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
    "\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n",
    "\"dst_host_srv_rerror_rate\",\"label\" ,\"difficulty\" ]\n",
    "\n",
    "data = pd.read_csv('data.txt', header=None)\n",
    "data.columns = columns\n",
    "\n",
    "print(data.head()) # printing the first 5 rows of the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "aa0ced81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125973 entries, 0 to 125972\n",
      "Data columns (total 43 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   duration                     125973 non-null  int64  \n",
      " 1   protocol_type                125973 non-null  object \n",
      " 2   service                      125973 non-null  object \n",
      " 3   flag                         125973 non-null  object \n",
      " 4   src_bytes                    125973 non-null  int64  \n",
      " 5   dst_bytes                    125973 non-null  int64  \n",
      " 6   land                         125973 non-null  int64  \n",
      " 7   wrong_fragment               125973 non-null  int64  \n",
      " 8   urgent                       125973 non-null  int64  \n",
      " 9   hot                          125973 non-null  int64  \n",
      " 10  num_failed_logins            125973 non-null  int64  \n",
      " 11  logged_in                    125973 non-null  int64  \n",
      " 12  num_compromised              125973 non-null  int64  \n",
      " 13  root_shell                   125973 non-null  int64  \n",
      " 14  su_attempted                 125973 non-null  int64  \n",
      " 15  num_root                     125973 non-null  int64  \n",
      " 16  num_file_creations           125973 non-null  int64  \n",
      " 17  num_shells                   125973 non-null  int64  \n",
      " 18  num_access_files             125973 non-null  int64  \n",
      " 19  num_outbound_cmds            125973 non-null  int64  \n",
      " 20  is_host_login                125973 non-null  int64  \n",
      " 21  is_guest_login               125973 non-null  int64  \n",
      " 22  count                        125973 non-null  int64  \n",
      " 23  srv_count                    125973 non-null  int64  \n",
      " 24  serror_rate                  125973 non-null  float64\n",
      " 25  srv_serror_rate              125973 non-null  float64\n",
      " 26  rerror_rate                  125973 non-null  float64\n",
      " 27  srv_rerror_rate              125973 non-null  float64\n",
      " 28  same_srv_rate                125973 non-null  float64\n",
      " 29  diff_srv_rate                125973 non-null  float64\n",
      " 30  srv_diff_host_rate           125973 non-null  float64\n",
      " 31  dst_host_count               125973 non-null  int64  \n",
      " 32  dst_host_srv_count           125973 non-null  int64  \n",
      " 33  dst_host_same_srv_rate       125973 non-null  float64\n",
      " 34  dst_host_diff_srv_rate       125973 non-null  float64\n",
      " 35  dst_host_same_src_port_rate  125973 non-null  float64\n",
      " 36  dst_host_srv_diff_host_rate  125973 non-null  float64\n",
      " 37  dst_host_serror_rate         125973 non-null  float64\n",
      " 38  dst_host_srv_serror_rate     125973 non-null  float64\n",
      " 39  dst_host_rerror_rate         125973 non-null  float64\n",
      " 40  dst_host_srv_rerror_rate     125973 non-null  float64\n",
      " 41  label                        125973 non-null  object \n",
      " 42  difficulty                   125973 non-null  int64  \n",
      "dtypes: float64(15), int64(24), object(4)\n",
      "memory usage: 41.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info() #displaying the data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "38604089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "normal             67343\n",
       "neptune            41214\n",
       "satan               3633\n",
       "ipsweep             3599\n",
       "portsweep           2931\n",
       "smurf               2646\n",
       "nmap                1493\n",
       "back                 956\n",
       "teardrop             892\n",
       "warezclient          890\n",
       "pod                  201\n",
       "guess_passwd          53\n",
       "buffer_overflow       30\n",
       "warezmaster           20\n",
       "land                  18\n",
       "imap                  11\n",
       "rootkit               10\n",
       "loadmodule             9\n",
       "ftp_write              8\n",
       "multihop               7\n",
       "phf                    4\n",
       "perl                   3\n",
       "spy                    2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts() # displaying all the normal samples and attack samples counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5b413",
   "metadata": {},
   "source": [
    "## 1. Why do we set random seeds? What is reproducibility in machine learning?\n",
    "- In machine learning, we set random seeds to ensure reproducibility, meaning the same code and data always produce the same results\n",
    "\n",
    "## 2. How many samples are in the dataset?\n",
    "- its 125973 from data.info()\n",
    "\n",
    "## 3. How many features are present (excluding label and difficulty)?\n",
    "- there are 41 total features(excluding label and difficulty) because from data.info() we get 43\n",
    "\n",
    "## 4. What is the distribution between normal samples and attacks?\n",
    "- Normal samples: 67,343 (53.4% of dataset)\n",
    "- Attack samples: 58,630 (46.6% of dataset)\n",
    "- Total: 125,973 samples\n",
    "- This is a ~53:47 ratio, which is pretty good for machine learning (not heavily imbalanced)\n",
    "## 5. Why is this distribution important for model training?\n",
    "\n",
    "- This distribution is important because the model learns patterns from the data, so if the training data doesn’t represent the real-world distribution, the model won’t perform well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e50032",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "81aee8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['difficulty'], axis=1, inplace=True) # dropping the difficulty column as it is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d7041939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = data.drop(['label'], axis=1) # features data frame without the label column\n",
    "Y_labels = data['label'] # labels data frame with only the label column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "05e3b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = pd.get_dummies(X_features, columns=['protocol_type', 'service', 'flag'])\n",
    "# One-hot encoding for categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "87bdb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = data['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "# converting labels to binary values: 0 for normal, 1 for attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "992d8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_features) # fit transform to scale the features\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_features.columns) # converting back to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0db0d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_labels, test_size=0.2, random_state=42) \n",
    "# splitting the data into training and testing sets with 80% training and 20% testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0500d",
   "metadata": {},
   "source": [
    "## 6. What encoding technique do you use for categorical variables? Why?\n",
    "- we use One-Hot Encoding (via pd.get_dummies()) because Neural networks need numbers, not text while avoiding false numerical relationships between categories.\n",
    "\n",
    "## 7. How many features do you have after encoding? Why did the number increase?\n",
    "\n",
    "- we have 122 features because One-hot encoding replaces each categorical column with multiple binary columns (one per unique value). \n",
    "\n",
    "## 8. What type of classification problem is this (binary, multi-class, multi-label)?\n",
    "\n",
    "- its a Binary Classification problem becasuse we encoded the labels as 0 (normal) or 1 (attack), creating only two possible classes.\n",
    "\n",
    "## 9. Why is feature scaling important for neural networks?\n",
    "- Without scaling, features with large values (e.g., src_bytes = 50,000) dominate the gradient updates over small-value features (e.g., serror_rate = 0.05), causing the model to ignore smaller features.\n",
    "\n",
    "## 10. How many samples are in the training and testing sets?\n",
    "- X_Train:100778\n",
    "- X_Test: 25195\n",
    "- Y_Train: 100778\n",
    "- Y_Test:25195"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82ce88",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the build function\n",
    "\n",
    "def build_model(n_hidden_layers, n_neurons, learning_rate, dropout_rate):\n",
    "    model = Sequential()\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(Dense(n_neurons, activation='relu', input_shape=(122,)))# adding hidden layers with relu activation function\n",
    "        model.add(Dropout(dropout_rate))# adding dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))  # output layer for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #compiling the model with adam optimizer and binary crossentropy loss function that is suitable for binary classification and the adam optimizer is an efficient optimization algorithm that adapts the learning rate during training.\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4edc97",
   "metadata": {},
   "source": [
    "## 12. Why use sigmoid activation for the output layer?\n",
    "- We use sigmoid activation for the output layer in binary classification because it squashes the output to a value between 0 and 1, which can be interpreted as the probability of the positive class.\n",
    "\n",
    "## 13. What is the purpose of dropout layers?\n",
    "- The purpose of dropout layers is to prevent overfitting in neural networks.\n",
    "\n",
    "## 14. Compare the two architectures:\n",
    "\n",
    "\n",
    "| Model           | Hidden Layers | Neurons per Layer | Trainable Parameters | Compression (Input → First Hidden) |\n",
    "|-----------------|---------------|-----------------|--------------------|----------------------------------|\n",
    "| Shallow Network | 1             | 4               | 497                | 122 → 4 ≈ 31                     |\n",
    "| Deep Network    | 3             | 32              | 6071               | 122 → 32 ≈ 3.8                   |\n",
    "\n",
    "## Notes\n",
    "- **Trainable parameters calculation:**  \n",
    "  - Dense layer: `(inputs × neurons) + neurons (biases)`  \n",
    "  - Shallow hidden layer: (122×4) + 4 = 492  \n",
    "  - Shallow output layer: (4×1) + 1 = 5 → Total = 492 + 5 = 497  \n",
    "  - Deep first hidden: (122×32) + 32 = 3936  \n",
    "  - Deep second hidden: (32×32) + 32 = 1056  \n",
    "  - Deep third hidden: (32×32) + 32 = 1056  \n",
    "  - Deep output: (32×1) + 1 = 33 → Total = 3936+1056+1056+33 = 6071  \n",
    "- **Compression ratio:**  \n",
    "  - Shallow: 122 input features → 4 neurons → 122 ÷ 4 ≈ 31  \n",
    "    (high compression → lots of input info squeezed)  \n",
    "  - Deep: 122 input features → 32 neurons → 122 ÷ 32 ≈ 3.8  \n",
    "    (less compression → preserves more info for deeper layers)\n",
    "\n",
    "## 15. Predict which model will perform better and explain why.\n",
    "\n",
    "The deep network (Model 2) will likely perform better.\n",
    "\n",
    "Why:\n",
    "\n",
    "- It has more hidden layers and neurons, so it can learn more complex patterns in the data.\n",
    "\n",
    "- Dropout prevents overfittin.\n",
    "\n",
    "- Smaller learning rate and batch size allow stable and precise training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
